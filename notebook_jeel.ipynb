{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56955ab4",
   "metadata": {},
   "source": [
    "# Assessing Wikipedia Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd016c",
   "metadata": {},
   "source": [
    "## 1. You will need to collect data from a source of your choosing (dataset, wikipedia API, web-scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376d2cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fcdac3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91972697",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4602f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8995c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words2</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>pos</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Orange</td>\n",
       "      <td>orange</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>New</td>\n",
       "      <td>new</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Black</td>\n",
       "      <td>black</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>star</td>\n",
       "      <td>star</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yael</td>\n",
       "      <td>yael</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence    outlet  \\\n",
       "0           0  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "1           3  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "2           4  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "3           5  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "4           6  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "\n",
       "         topic   type                                            article  \\\n",
       "0  environment  right  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "1  environment  right  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "2  environment  right  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "3  environment  right  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "4  environment  right  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "\n",
       "  biased_words2    text text_low    pos  ... ne_NORP_context  \\\n",
       "0            []  Orange   orange  PROPN  ...               0   \n",
       "1            []     New      new  PROPN  ...               0   \n",
       "2            []   Black    black  PROPN  ...               0   \n",
       "3            []    star     star   NOUN  ...               0   \n",
       "4            []    Yael     yael  PROPN  ...               0   \n",
       "\n",
       "  ne_ORDINAL_context ne_ORG_context ne_PERCENT_context  ne_PERSON_context  \\\n",
       "0                  0              0                  0                  0   \n",
       "1                  0              0                  0                  0   \n",
       "2                  0              0                  0                  1   \n",
       "3                  0              0                  0                  1   \n",
       "4                  0              0                  0                  1   \n",
       "\n",
       "   ne_PRODUCT_context  ne_QUANTITY_context  ne_TIME_context  \\\n",
       "0                   0                    0                0   \n",
       "1                   0                    0                0   \n",
       "2                   0                    0                0   \n",
       "3                   0                    0                0   \n",
       "4                   0                    0                0   \n",
       "\n",
       "   ne_WORK_OF_ART_context  ne_LANGUAGE_context  \n",
       "0                       1                    0  \n",
       "1                       1                    0  \n",
       "2                       1                    0  \n",
       "3                       1                    0  \n",
       "4                       1                    0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data = pd.read_excel('data.xlsx')\n",
    "# Display the first few rows of the dataset\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5b3d0",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0d5b3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'sentence',\n",
       " 'outlet',\n",
       " 'topic',\n",
       " 'type',\n",
       " 'article',\n",
       " 'biased_words2',\n",
       " 'text',\n",
       " 'text_low',\n",
       " 'pos',\n",
       " 'lemma',\n",
       " 'lemma_low',\n",
       " 'tag',\n",
       " 'dep',\n",
       " 'is_stop',\n",
       " 'glove_vec300_norm',\n",
       " 'order',\n",
       " 'tfidf_art',\n",
       " 'label3',\n",
       " 'label4',\n",
       " 'label5',\n",
       " 'is_ne',\n",
       " 'ne_label',\n",
       " 'negative_conc',\n",
       " 'positive_conc',\n",
       " 'weak_subj',\n",
       " 'strong_subj',\n",
       " 'MRCP_concretness_ratings',\n",
       " 'MRCP_Imagability_ratings',\n",
       " 'hyperbolic_terms',\n",
       " 'attitude_markers',\n",
       " 'kill_verbs',\n",
       " 'bias_lexicon',\n",
       " 'assertive_verbs',\n",
       " 'factive_verbs',\n",
       " 'report_verbs',\n",
       " 'implicative_verbs',\n",
       " 'hedges',\n",
       " 'boosters',\n",
       " 'affect ',\n",
       " 'posemo ',\n",
       " 'negemo ',\n",
       " 'anx ',\n",
       " 'anger ',\n",
       " 'sad ',\n",
       " 'social ',\n",
       " 'family ',\n",
       " 'friend ',\n",
       " 'female ',\n",
       " 'male ',\n",
       " 'cogproc ',\n",
       " 'insight ',\n",
       " 'cause ',\n",
       " 'discrep ',\n",
       " 'tentat ',\n",
       " 'certain ',\n",
       " 'differ ',\n",
       " 'percept ',\n",
       " 'see ',\n",
       " 'hear ',\n",
       " 'feel ',\n",
       " 'bio ',\n",
       " 'body ',\n",
       " 'health ',\n",
       " 'sexual ',\n",
       " 'ingest ',\n",
       " 'drives ',\n",
       " 'affiliation ',\n",
       " 'achieve ',\n",
       " 'power ',\n",
       " 'reward ',\n",
       " 'risk ',\n",
       " 'focuspast ',\n",
       " 'focuspresent ',\n",
       " 'focusfuture ',\n",
       " 'relativ ',\n",
       " 'motion ',\n",
       " 'space ',\n",
       " 'time ',\n",
       " 'work ',\n",
       " 'leisure ',\n",
       " 'home ',\n",
       " 'money ',\n",
       " 'relig ',\n",
       " 'death ',\n",
       " 'informal ',\n",
       " 'swear ',\n",
       " 'netspeak ',\n",
       " 'assent ',\n",
       " 'nonflu ',\n",
       " 'filler ',\n",
       " 'pos_ADJ',\n",
       " 'pos_ADP',\n",
       " 'pos_ADV',\n",
       " 'pos_AUX',\n",
       " 'pos_DET',\n",
       " 'pos_INTJ',\n",
       " 'pos_NOUN',\n",
       " 'pos_PRON',\n",
       " 'pos_PROPN',\n",
       " 'pos_SCONJ',\n",
       " 'pos_VERB',\n",
       " 'pos_X',\n",
       " 'dep_ROOT',\n",
       " 'dep_acl',\n",
       " 'dep_acomp',\n",
       " 'dep_advcl',\n",
       " 'dep_advmod',\n",
       " 'dep_agent',\n",
       " 'dep_amod',\n",
       " 'dep_appos',\n",
       " 'dep_attr',\n",
       " 'dep_aux',\n",
       " 'dep_auxpass',\n",
       " 'dep_cc',\n",
       " 'dep_ccomp',\n",
       " 'dep_compound',\n",
       " 'dep_conj',\n",
       " 'dep_csubj',\n",
       " 'dep_dative',\n",
       " 'dep_dep',\n",
       " 'dep_det',\n",
       " 'dep_dobj',\n",
       " 'dep_expl',\n",
       " 'dep_intj',\n",
       " 'dep_mark',\n",
       " 'dep_neg',\n",
       " 'dep_nmod',\n",
       " 'dep_npadvmod',\n",
       " 'dep_nsubj',\n",
       " 'dep_nsubjpass',\n",
       " 'dep_nummod',\n",
       " 'dep_oprd',\n",
       " 'dep_parataxis',\n",
       " 'dep_pcomp',\n",
       " 'dep_pobj',\n",
       " 'dep_poss',\n",
       " 'dep_preconj',\n",
       " 'dep_predet',\n",
       " 'dep_prep',\n",
       " 'dep_prt',\n",
       " 'dep_punct',\n",
       " 'dep_quantmod',\n",
       " 'dep_relcl',\n",
       " 'dep_xcomp',\n",
       " 'ne_CARDINAL',\n",
       " 'ne_DATE',\n",
       " 'ne_EVENT',\n",
       " 'ne_FAC',\n",
       " 'ne_GPE',\n",
       " 'ne_LANGUAGE',\n",
       " 'ne_LAW',\n",
       " 'ne_LOC',\n",
       " 'ne_MONEY',\n",
       " 'ne_NORP',\n",
       " 'ne_ORDINAL',\n",
       " 'ne_ORG',\n",
       " 'ne_PERCENT',\n",
       " 'ne_PERSON',\n",
       " 'ne_PRODUCT',\n",
       " 'ne_QUANTITY',\n",
       " 'ne_TIME',\n",
       " 'ne_WORK_OF_ART',\n",
       " 'negative_conc_context',\n",
       " 'positive_conc_context',\n",
       " 'weak_subj_context',\n",
       " 'strong_subj_context',\n",
       " 'hyperbolic_terms_context',\n",
       " 'attitude_markers_context',\n",
       " 'kill_verbs_context',\n",
       " 'bias_lexicon_context',\n",
       " 'assertive_verbs_context',\n",
       " 'factive_verbs_context',\n",
       " 'report_verbs_context',\n",
       " 'implicative_verbs_context',\n",
       " 'hedges_context',\n",
       " 'boosters_context',\n",
       " 'affect _context',\n",
       " 'posemo _context',\n",
       " 'negemo _context',\n",
       " 'anx _context',\n",
       " 'anger _context',\n",
       " 'sad _context',\n",
       " 'social _context',\n",
       " 'family _context',\n",
       " 'friend _context',\n",
       " 'female _context',\n",
       " 'male _context',\n",
       " 'cogproc _context',\n",
       " 'insight _context',\n",
       " 'cause _context',\n",
       " 'discrep _context',\n",
       " 'tentat _context',\n",
       " 'certain _context',\n",
       " 'differ _context',\n",
       " 'percept _context',\n",
       " 'see _context',\n",
       " 'hear _context',\n",
       " 'feel _context',\n",
       " 'bio _context',\n",
       " 'body _context',\n",
       " 'health _context',\n",
       " 'sexual _context',\n",
       " 'ingest _context',\n",
       " 'drives _context',\n",
       " 'affiliation _context',\n",
       " 'achieve _context',\n",
       " 'power _context',\n",
       " 'reward _context',\n",
       " 'risk _context',\n",
       " 'focuspast _context',\n",
       " 'focuspresent _context',\n",
       " 'focusfuture _context',\n",
       " 'relativ _context',\n",
       " 'motion _context',\n",
       " 'space _context',\n",
       " 'time _context',\n",
       " 'work _context',\n",
       " 'leisure _context',\n",
       " 'home _context',\n",
       " 'money _context',\n",
       " 'relig _context',\n",
       " 'death _context',\n",
       " 'informal _context',\n",
       " 'swear _context',\n",
       " 'netspeak _context',\n",
       " 'assent _context',\n",
       " 'nonflu _context',\n",
       " 'filler _context',\n",
       " 'pos_ADJ_context',\n",
       " 'pos_ADP_context',\n",
       " 'pos_ADV_context',\n",
       " 'pos_AUX_context',\n",
       " 'pos_DET_context',\n",
       " 'pos_INTJ_context',\n",
       " 'pos_NOUN_context',\n",
       " 'pos_PRON_context',\n",
       " 'pos_PROPN_context',\n",
       " 'pos_SCONJ_context',\n",
       " 'pos_VERB_context',\n",
       " 'pos_X_context',\n",
       " 'dep_ROOT_context',\n",
       " 'dep_acl_context',\n",
       " 'dep_acomp_context',\n",
       " 'dep_advcl_context',\n",
       " 'dep_advmod_context',\n",
       " 'dep_agent_context',\n",
       " 'dep_amod_context',\n",
       " 'dep_appos_context',\n",
       " 'dep_attr_context',\n",
       " 'dep_aux_context',\n",
       " 'dep_auxpass_context',\n",
       " 'dep_cc_context',\n",
       " 'dep_ccomp_context',\n",
       " 'dep_compound_context',\n",
       " 'dep_conj_context',\n",
       " 'dep_csubj_context',\n",
       " 'dep_dative_context',\n",
       " 'dep_dep_context',\n",
       " 'dep_det_context',\n",
       " 'dep_dobj_context',\n",
       " 'dep_expl_context',\n",
       " 'dep_intj_context',\n",
       " 'dep_mark_context',\n",
       " 'dep_neg_context',\n",
       " 'dep_nmod_context',\n",
       " 'dep_npadvmod_context',\n",
       " 'dep_nsubj_context',\n",
       " 'dep_nsubjpass_context',\n",
       " 'dep_nummod_context',\n",
       " 'dep_oprd_context',\n",
       " 'dep_parataxis_context',\n",
       " 'dep_pcomp_context',\n",
       " 'dep_pobj_context',\n",
       " 'dep_poss_context',\n",
       " 'dep_preconj_context',\n",
       " 'dep_predet_context',\n",
       " 'dep_prep_context',\n",
       " 'dep_prt_context',\n",
       " 'dep_punct_context',\n",
       " 'dep_quantmod_context',\n",
       " 'dep_relcl_context',\n",
       " 'dep_xcomp_context',\n",
       " 'ne_CARDINAL_context',\n",
       " 'ne_DATE_context',\n",
       " 'ne_EVENT_context',\n",
       " 'ne_FAC_context',\n",
       " 'ne_GPE_context',\n",
       " 'ne_LAW_context',\n",
       " 'ne_LOC_context',\n",
       " 'ne_MONEY_context',\n",
       " 'ne_NORP_context',\n",
       " 'ne_ORDINAL_context',\n",
       " 'ne_ORG_context',\n",
       " 'ne_PERCENT_context',\n",
       " 'ne_PERSON_context',\n",
       " 'ne_PRODUCT_context',\n",
       " 'ne_QUANTITY_context',\n",
       " 'ne_TIME_context',\n",
       " 'ne_WORK_OF_ART_context',\n",
       " 'ne_LANGUAGE_context']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the column names of the dataset\n",
    "column_names = data.columns.tolist()\n",
    "display(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebfa8dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 65822 rows and 301 columns\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the dataset\n",
    "n_rows, n_cols = data.shape\n",
    "print(f\"The DataFrame has {n_rows} rows and {n_cols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec3998e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65822 entries, 0 to 65821\n",
      "Columns: 301 entries, Unnamed: 0 to ne_LANGUAGE_context\n",
      "dtypes: bool(2), float64(4), int64(281), object(14)\n",
      "memory usage: 150.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display the informative summary of the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "486f9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>glove_vec300_norm</th>\n",
       "      <th>order</th>\n",
       "      <th>tfidf_art</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>negative_conc</th>\n",
       "      <th>positive_conc</th>\n",
       "      <th>weak_subj</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>64632.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "      <td>65822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54925.591641</td>\n",
       "      <td>7.616181</td>\n",
       "      <td>16.466030</td>\n",
       "      <td>0.209871</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.126447</td>\n",
       "      <td>0.117362</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071390</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.130078</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.132889</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.008325</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31794.881230</td>\n",
       "      <td>0.864257</td>\n",
       "      <td>11.775391</td>\n",
       "      <td>0.068478</td>\n",
       "      <td>0.221822</td>\n",
       "      <td>0.221822</td>\n",
       "      <td>0.221822</td>\n",
       "      <td>0.332355</td>\n",
       "      <td>0.321854</td>\n",
       "      <td>0.366051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257476</td>\n",
       "      <td>0.073753</td>\n",
       "      <td>0.336392</td>\n",
       "      <td>0.056126</td>\n",
       "      <td>0.339457</td>\n",
       "      <td>0.043537</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.090864</td>\n",
       "      <td>0.072933</td>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.004817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27253.250000</td>\n",
       "      <td>6.995967</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.163616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54976.500000</td>\n",
       "      <td>7.550039</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.202041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>82310.500000</td>\n",
       "      <td>8.141136</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.245051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>110350.000000</td>\n",
       "      <td>11.844529</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.699726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  glove_vec300_norm         order     tfidf_art  \\\n",
       "count   65822.000000       65822.000000  65822.000000  64632.000000   \n",
       "mean    54925.591641           7.616181     16.466030      0.209871   \n",
       "std     31794.881230           0.864257     11.775391      0.068478   \n",
       "min         0.000000           5.004817      0.000000      0.000000   \n",
       "25%     27253.250000           6.995967      7.000000      0.163616   \n",
       "50%     54976.500000           7.550039     15.000000      0.202041   \n",
       "75%     82310.500000           8.141136     24.000000      0.245051   \n",
       "max    110350.000000          11.844529     89.000000      0.699726   \n",
       "\n",
       "             label3        label4        label5  negative_conc  positive_conc  \\\n",
       "count  65822.000000  65822.000000  65822.000000   65822.000000   65822.000000   \n",
       "mean       0.051898      0.051898      0.051898       0.126447       0.117362   \n",
       "std        0.221822      0.221822      0.221822       0.332355       0.321854   \n",
       "min        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
       "25%        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
       "50%        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
       "75%        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
       "max        1.000000      1.000000      1.000000       1.000000       1.000000   \n",
       "\n",
       "          weak_subj  ...  ne_NORP_context  ne_ORDINAL_context  ne_ORG_context  \\\n",
       "count  65822.000000  ...     65822.000000        65822.000000    65822.000000   \n",
       "mean       0.159400  ...         0.071390            0.005469        0.130078   \n",
       "std        0.366051  ...         0.257476            0.073753        0.336392   \n",
       "min        0.000000  ...         0.000000            0.000000        0.000000   \n",
       "25%        0.000000  ...         0.000000            0.000000        0.000000   \n",
       "50%        0.000000  ...         0.000000            0.000000        0.000000   \n",
       "75%        0.000000  ...         0.000000            0.000000        0.000000   \n",
       "max        1.000000  ...         1.000000            1.000000        1.000000   \n",
       "\n",
       "       ne_PERCENT_context  ne_PERSON_context  ne_PRODUCT_context  \\\n",
       "count        65822.000000       65822.000000        65822.000000   \n",
       "mean             0.003160           0.132889            0.001899   \n",
       "std              0.056126           0.339457            0.043537   \n",
       "min              0.000000           0.000000            0.000000   \n",
       "25%              0.000000           0.000000            0.000000   \n",
       "50%              0.000000           0.000000            0.000000   \n",
       "75%              0.000000           0.000000            0.000000   \n",
       "max              1.000000           1.000000            1.000000   \n",
       "\n",
       "       ne_QUANTITY_context  ne_TIME_context  ne_WORK_OF_ART_context  \\\n",
       "count         65822.000000     65822.000000            65822.000000   \n",
       "mean              0.000471         0.008325                0.005348   \n",
       "std               0.021697         0.090864                0.072933   \n",
       "min               0.000000         0.000000                0.000000   \n",
       "25%               0.000000         0.000000                0.000000   \n",
       "50%               0.000000         0.000000                0.000000   \n",
       "75%               0.000000         0.000000                0.000000   \n",
       "max               1.000000         1.000000                1.000000   \n",
       "\n",
       "       ne_LANGUAGE_context  \n",
       "count         65822.000000  \n",
       "mean              0.000091  \n",
       "std               0.009547  \n",
       "min               0.000000  \n",
       "25%               0.000000  \n",
       "50%               0.000000  \n",
       "75%               0.000000  \n",
       "max               1.000000  \n",
       "\n",
       "[8 rows x 285 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the descriptive statistics of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc2dbd",
   "metadata": {},
   "source": [
    "## 2. You will conduct EDA that you see fit to appropriately investigate text of wikipedia articles you look to predict on for biased terms, sentiment, or other linguistic significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910da4e2",
   "metadata": {},
   "source": [
    "## Explorating Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9456da3",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "998ad12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of duplicated data: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the number of duplicates in the dataset\n",
    "duplicates = data[data.duplicated()]\n",
    "display(f\"Number of duplicated data: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46d36b",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f75c2a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                    0\n",
       "sentence                      0\n",
       "outlet                        0\n",
       "topic                         0\n",
       "type                      16206\n",
       "                          ...  \n",
       "ne_PRODUCT_context            0\n",
       "ne_QUANTITY_context           0\n",
       "ne_TIME_context               0\n",
       "ne_WORK_OF_ART_context        0\n",
       "ne_LANGUAGE_context           0\n",
       "Length: 301, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                0.000000\n",
       "sentence                  0.000000\n",
       "outlet                    0.000000\n",
       "topic                     0.000000\n",
       "type                      0.246209\n",
       "                            ...   \n",
       "ne_PRODUCT_context        0.000000\n",
       "ne_QUANTITY_context       0.000000\n",
       "ne_TIME_context           0.000000\n",
       "ne_WORK_OF_ART_context    0.000000\n",
       "ne_LANGUAGE_context       0.000000\n",
       "Length: 301, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the number of missing values in the dataset\n",
    "display(data.isna().sum())\n",
    "\n",
    "# Check for missing values in the DataFrame as a percentage\n",
    "display(data.isna().sum()/len(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40cde572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the 'type' column\n",
    "data.dropna(subset=['type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b12601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                0.0\n",
       "sentence                  0.0\n",
       "outlet                    0.0\n",
       "topic                     0.0\n",
       "type                      0.0\n",
       "                         ... \n",
       "ne_PRODUCT_context        0.0\n",
       "ne_QUANTITY_context       0.0\n",
       "ne_TIME_context           0.0\n",
       "ne_WORK_OF_ART_context    0.0\n",
       "ne_LANGUAGE_context       0.0\n",
       "Length: 301, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values in the DataFrame as a percentage\n",
    "display(data.isna().sum()/len(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81246efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text data in the 'text' column\n",
    "# Define a function to clean the text data \n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\",\"\", text)\n",
    "    text = text.split()\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abffc4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words2</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50863</th>\n",
       "      <td>84833</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>trump-presidency</td>\n",
       "      <td>center</td>\n",
       "      <td>The president left the composition of the task...</td>\n",
       "      <td>[]</td>\n",
       "      <td>composition</td>\n",
       "      <td>composition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>composition</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the president left the composition of the task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52675</th>\n",
       "      <td>87921</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>The Trump administration gave the Border Patro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>asylum</td>\n",
       "      <td>asylum</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>asylum</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the trump administration gave the border patro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37890</th>\n",
       "      <td>63193</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>universal health care</td>\n",
       "      <td>left</td>\n",
       "      <td>President Trump, who repeatedly has lied to th...</td>\n",
       "      <td>['claiming', 'lied']</td>\n",
       "      <td>working</td>\n",
       "      <td>working</td>\n",
       "      <td>VERB</td>\n",
       "      <td>work</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>president trump who repeatedly has lied to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>41703</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>black lives matter</td>\n",
       "      <td>center</td>\n",
       "      <td>In recent weeks, Saks’ problems have been comp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>included</td>\n",
       "      <td>included</td>\n",
       "      <td>VERB</td>\n",
       "      <td>include</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in recent weeks saks problems have been compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53951</th>\n",
       "      <td>89975</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>abortion</td>\n",
       "      <td>left</td>\n",
       "      <td>The votes come as a new conservative majority ...</td>\n",
       "      <td>['nervous']</td>\n",
       "      <td>come</td>\n",
       "      <td>come</td>\n",
       "      <td>VERB</td>\n",
       "      <td>come</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the votes come as a new conservative majority ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     outlet                  topic    type  \\\n",
       "50863       84833  USA Today       trump-presidency  center   \n",
       "52675       87921   Alternet            immigration    left   \n",
       "37890       63193   Alternet  universal health care    left   \n",
       "25002       41703    Reuters     black lives matter  center   \n",
       "53951       89975      MSNBC               abortion    left   \n",
       "\n",
       "                                                 article  \\\n",
       "50863  The president left the composition of the task...   \n",
       "52675  The Trump administration gave the Border Patro...   \n",
       "37890  President Trump, who repeatedly has lied to th...   \n",
       "25002  In recent weeks, Saks’ problems have been comp...   \n",
       "53951  The votes come as a new conservative majority ...   \n",
       "\n",
       "              biased_words2         text     text_low   pos        lemma  ...  \\\n",
       "50863                    []  composition  composition  NOUN  composition  ...   \n",
       "52675                    []       asylum       asylum  NOUN       asylum  ...   \n",
       "37890  ['claiming', 'lied']      working      working  VERB         work  ...   \n",
       "25002                    []     included     included  VERB      include  ...   \n",
       "53951           ['nervous']         come         come  VERB         come  ...   \n",
       "\n",
       "      ne_ORDINAL_context ne_ORG_context ne_PERCENT_context  ne_PERSON_context  \\\n",
       "50863                  0              0                  0                  0   \n",
       "52675                  0              0                  0                  0   \n",
       "37890                  0              0                  0                  0   \n",
       "25002                  0              0                  0                  0   \n",
       "53951                  0              0                  0                  0   \n",
       "\n",
       "       ne_PRODUCT_context  ne_QUANTITY_context  ne_TIME_context  \\\n",
       "50863                   0                    0                0   \n",
       "52675                   0                    0                0   \n",
       "37890                   0                    0                0   \n",
       "25002                   0                    0                0   \n",
       "53951                   0                    0                0   \n",
       "\n",
       "       ne_WORK_OF_ART_context  ne_LANGUAGE_context  \\\n",
       "50863                       0                    0   \n",
       "52675                       0                    0   \n",
       "37890                       0                    0   \n",
       "25002                       0                    0   \n",
       "53951                       0                    0   \n",
       "\n",
       "                                              clean_text  \n",
       "50863  the president left the composition of the task...  \n",
       "52675  the trump administration gave the border patro...  \n",
       "37890  president trump who repeatedly has lied to the...  \n",
       "25002  in recent weeks saks problems have been compou...  \n",
       "53951  the votes come as a new conservative majority ...  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the clear_text function to the 'comment_text' column\n",
    "data['clean_text'] = data['sentence'].astype(str).apply(clear_text) \n",
    "data= data.drop(columns=['sentence'])\n",
    "\n",
    "# Display the first 5 rows of the comments DataFrame after cleaning\n",
    "display(data.sample(5)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41b0b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(data['clean_text'].isna().sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9744ad9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49616 entries, 0 to 65821\n",
      "Columns: 301 entries, Unnamed: 0 to clean_text\n",
      "dtypes: bool(2), float64(4), int64(281), object(14)\n",
      "memory usage: 113.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16c8b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set of English stop words\n",
    "stop_words =  set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f458c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57095114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clear_text function to the 'comment_text' column\n",
    "data['lemmatize_text'] = data['clean_text'].apply(lemmatize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e568e71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40603</th>\n",
       "      <td>schlapps apology comes as the us is convulsed ...</td>\n",
       "      <td>schlapps apology come u convulsed protest poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>a us official speaking on condition of anonymi...</td>\n",
       "      <td>u official speaking condition anonymity confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34458</th>\n",
       "      <td>once powerful hollywood producer harvey weinst...</td>\n",
       "      <td>powerful hollywood producer harvey weinstein c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48880</th>\n",
       "      <td>the leftwing mob had gathered in parliament sq...</td>\n",
       "      <td>leftwing mob gathered parliament square london...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36679</th>\n",
       "      <td>president donald trump has characterized those...</td>\n",
       "      <td>president donald trump characterized clashing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  \\\n",
       "40603  schlapps apology comes as the us is convulsed ...   \n",
       "2548   a us official speaking on condition of anonymi...   \n",
       "34458  once powerful hollywood producer harvey weinst...   \n",
       "48880  the leftwing mob had gathered in parliament sq...   \n",
       "36679  president donald trump has characterized those...   \n",
       "\n",
       "                                          lemmatize_text  \n",
       "40603  schlapps apology come u convulsed protest poli...  \n",
       "2548   u official speaking condition anonymity confir...  \n",
       "34458  powerful hollywood producer harvey weinstein c...  \n",
       "48880  leftwing mob gathered parliament square london...  \n",
       "36679  president donald trump characterized clashing ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first 5 rows of the comments DataFrame after cleaning\n",
    "display(data[['clean_text', 'lemmatize_text']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a1c4e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49616, 302)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a9600",
   "metadata": {},
   "source": [
    "## 3. You will conduct supervised learning to be able to predict if a given text is biased. You might want to be able to do this on the sentence by sentence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0321534",
   "metadata": {},
   "source": [
    "## 4. You need to have a prediction function that can take in a new wikipedia article and predict how biased it is. You can do this by predicting if each sentence in an article is biased, then perhaps scaling the results by the length of the article to get somewhat of a“bias score”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
