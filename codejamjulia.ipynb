{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:20.326841Z",
     "iopub.status.busy": "2025-05-04T14:20:20.326015Z",
     "iopub.status.idle": "2025-05-04T14:20:20.334636Z",
     "shell.execute_reply": "2025-05-04T14:20:20.333442Z",
     "shell.execute_reply.started": "2025-05-04T14:20:20.326804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:20.337486Z",
     "iopub.status.busy": "2025-05-04T14:20:20.337109Z",
     "iopub.status.idle": "2025-05-04T14:20:32.356471Z",
     "shell.execute_reply": "2025-05-04T14:20:32.355379Z",
     "shell.execute_reply.started": "2025-05-04T14:20:20.337442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import libraries to be used \n",
    "\n",
    "#general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#machine learning model requirements\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, accuracy_score,\n",
    "    precision_recall_curve, roc_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:32.358437Z",
     "iopub.status.busy": "2025-05-04T14:20:32.357656Z",
     "iopub.status.idle": "2025-05-04T14:20:35.041691Z",
     "shell.execute_reply": "2025-05-04T14:20:35.040793Z",
     "shell.execute_reply.started": "2025-05-04T14:20:32.358400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List all files in the BABE dataset\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.xlsx'):\n",
    "            excel_path = os.path.join(dirname, filename)\n",
    "            print(\"Excel file found:\", excel_path)\n",
    "\n",
    "            # List all sheet names in that file\n",
    "            xls = pd.ExcelFile(excel_path)\n",
    "            print(\"Sheet names:\", xls.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:35.043876Z",
     "iopub.status.busy": "2025-05-04T14:20:35.042684Z",
     "iopub.status.idle": "2025-05-04T14:24:47.370305Z",
     "shell.execute_reply": "2025-05-04T14:24:47.369372Z",
     "shell.execute_reply.started": "2025-05-04T14:20:35.043849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load just one worksheet by name\n",
    "df = pd.read_excel('dt_final_SG2.xlsx', sheet_name='Sheet1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.372828Z",
     "iopub.status.busy": "2025-05-04T14:24:47.372562Z",
     "iopub.status.idle": "2025-05-04T14:24:47.380892Z",
     "shell.execute_reply": "2025-05-04T14:24:47.379765Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.372809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sentence', 'outlet', 'topic', 'type', 'article',\n",
       "       'biased_words2', 'text', 'text_low', 'pos',\n",
       "       ...\n",
       "       'ne_NORP_context', 'ne_ORDINAL_context', 'ne_ORG_context',\n",
       "       'ne_PERCENT_context', 'ne_PERSON_context', 'ne_PRODUCT_context',\n",
       "       'ne_QUANTITY_context', 'ne_TIME_context', 'ne_WORK_OF_ART_context',\n",
       "       'ne_LANGUAGE_context'],\n",
       "      dtype='object', length=301)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.382381Z",
     "iopub.status.busy": "2025-05-04T14:24:47.381969Z",
     "iopub.status.idle": "2025-05-04T14:24:47.405447Z",
     "shell.execute_reply": "2025-05-04T14:24:47.404556Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.382347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_file(file_path, duplicate_column=None):\n",
    "    \"\"\"\n",
    "    Evaluates a given CSV or Excel file by:\n",
    "    - Printing general file information\n",
    "    - Checking for duplicate values in a specified column\n",
    "    - Searching for zero values\n",
    "    - Searching for empty (NaN) cells\n",
    "    \"\"\"\n",
    "\n",
    "    # Print file information\n",
    "    print(\"\\n=== File Info ===\")\n",
    "    print(file_path.info())\n",
    "\n",
    "    # Check for duplicates in the specified column\n",
    "    if duplicate_column:\n",
    "        duplicate_count = file_path.duplicated(subset=[duplicate_column]).sum()\n",
    "        print(f\"\\n=== Duplicates in '{duplicate_column}' ===\")\n",
    "        print(f\"Total duplicate values: {duplicate_count}\")\n",
    "    \n",
    "    # Check for zero values\n",
    "    zero_values = (file_path == 0).sum().sum()\n",
    "    print(f\"\\n=== Zero Values ===\")\n",
    "    print(f\"Total zero values: {zero_values}\")\n",
    "\n",
    "    # Check for empty (NaN) cells\n",
    "    missing_values = file_path.isnull().sum().sum()\n",
    "    print(f\"\\n=== Empty (NaN) Cells ===\")\n",
    "    print(f\"Total empty cells: {missing_values}\")\n",
    "    \n",
    "    #print a sample \n",
    "    display(file_path.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.406588Z",
     "iopub.status.busy": "2025-05-04T14:24:47.406310Z",
     "iopub.status.idle": "2025-05-04T14:24:47.798299Z",
     "shell.execute_reply": "2025-05-04T14:24:47.797358Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.406556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== File Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65822 entries, 0 to 65821\n",
      "Columns: 301 entries, Unnamed: 0 to ne_LANGUAGE_context\n",
      "dtypes: bool(2), float64(4), int64(281), object(14)\n",
      "memory usage: 150.3+ MB\n",
      "None\n",
      "\n",
      "=== Duplicates in 'text' ===\n",
      "Total duplicate values: 52531\n",
      "\n",
      "=== Zero Values ===\n",
      "Total zero values: 17437766\n",
      "\n",
      "=== Empty (NaN) Cells ===\n",
      "Total empty cells: 140258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words2</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>pos</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>8322</td>\n",
       "      <td>An analysis by Fox News shows at least eight p...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>middle-class</td>\n",
       "      <td>right</td>\n",
       "      <td>An analysis by Fox News shows at least eight p...</td>\n",
       "      <td>['drastic', 'overhaul']</td>\n",
       "      <td>system</td>\n",
       "      <td>system</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53726</th>\n",
       "      <td>89616</td>\n",
       "      <td>The United States should learn from China and ...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>gun control</td>\n",
       "      <td>center</td>\n",
       "      <td>The United States should learn from China and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>China</td>\n",
       "      <td>china</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29151</th>\n",
       "      <td>48769</td>\n",
       "      <td>Leonhardt’s column in the New York Times provi...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>middle-class</td>\n",
       "      <td>right</td>\n",
       "      <td>Leonhardt’s column in the New York Times provi...</td>\n",
       "      <td>['enamored', 'chaotic', 'fractured']</td>\n",
       "      <td>column</td>\n",
       "      <td>column</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64523</th>\n",
       "      <td>108111</td>\n",
       "      <td>White supremacist violent extremists can gener...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>White supremacist violent extremists can gener...</td>\n",
       "      <td>['violent', 'virulent', 'supremacist', 'hatred']</td>\n",
       "      <td>anti</td>\n",
       "      <td>anti</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>238</td>\n",
       "      <td>[The police] now prefer to think of themselves...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>marriage-equality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[The police] now prefer to think of themselves...</td>\n",
       "      <td>['wounds']</td>\n",
       "      <td>prance</td>\n",
       "      <td>prance</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           sentence  \\\n",
       "5011         8322  An analysis by Fox News shows at least eight p...   \n",
       "53726       89616  The United States should learn from China and ...   \n",
       "29151       48769  Leonhardt’s column in the New York Times provi...   \n",
       "64523      108111  White supremacist violent extremists can gener...   \n",
       "150           238  [The police] now prefer to think of themselves...   \n",
       "\n",
       "          outlet              topic    type  \\\n",
       "5011    Fox News       middle-class   right   \n",
       "53726    Reuters        gun control  center   \n",
       "29151  Breitbart       middle-class   right   \n",
       "64523      MSNBC  white-nationalism    left   \n",
       "150    Breitbart  marriage-equality     NaN   \n",
       "\n",
       "                                                 article  \\\n",
       "5011   An analysis by Fox News shows at least eight p...   \n",
       "53726  The United States should learn from China and ...   \n",
       "29151  Leonhardt’s column in the New York Times provi...   \n",
       "64523  White supremacist violent extremists can gener...   \n",
       "150    [The police] now prefer to think of themselves...   \n",
       "\n",
       "                                          biased_words2    text text_low  \\\n",
       "5011                            ['drastic', 'overhaul']  system   system   \n",
       "53726                                                []   China    china   \n",
       "29151              ['enamored', 'chaotic', 'fractured']  column   column   \n",
       "64523  ['violent', 'virulent', 'supremacist', 'hatred']    anti     anti   \n",
       "150                                          ['wounds']  prance   prance   \n",
       "\n",
       "         pos  ... ne_NORP_context ne_ORDINAL_context ne_ORG_context  \\\n",
       "5011    NOUN  ...               0                  0              0   \n",
       "53726  PROPN  ...               0                  0              0   \n",
       "29151   NOUN  ...               0                  0              1   \n",
       "64523   NOUN  ...               1                  0              0   \n",
       "150     VERB  ...               0                  0              0   \n",
       "\n",
       "      ne_PERCENT_context  ne_PERSON_context  ne_PRODUCT_context  \\\n",
       "5011                   0                  0                   0   \n",
       "53726                  0                  0                   0   \n",
       "29151                  0                  0                   0   \n",
       "64523                  0                  0                   0   \n",
       "150                    0                  0                   0   \n",
       "\n",
       "       ne_QUANTITY_context  ne_TIME_context  ne_WORK_OF_ART_context  \\\n",
       "5011                     0                0                       0   \n",
       "53726                    0                0                       0   \n",
       "29151                    0                0                       0   \n",
       "64523                    0                0                       0   \n",
       "150                      0                0                       0   \n",
       "\n",
       "       ne_LANGUAGE_context  \n",
       "5011                     0  \n",
       "53726                    0  \n",
       "29151                    0  \n",
       "64523                    0  \n",
       "150                      0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run pipeline for each of the files to evaluate issues \n",
    "evaluate_file(df, duplicate_column=\"text\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.799200Z",
     "iopub.status.busy": "2025-05-04T14:24:47.798953Z",
     "iopub.status.idle": "2025-05-04T14:24:47.864787Z",
     "shell.execute_reply": "2025-05-04T14:24:47.863975Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.799179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['type', 'tfidf_art', 'ne_label', 'MRCP_concretness_ratings', 'MRCP_Imagability_ratings']\n"
     ]
    }
   ],
   "source": [
    "# Show columns with at least one NaN value\n",
    "nan_columns = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "print(\"Columns with NaN values:\", nan_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.865868Z",
     "iopub.status.busy": "2025-05-04T14:24:47.865651Z",
     "iopub.status.idle": "2025-05-04T14:24:47.973002Z",
     "shell.execute_reply": "2025-05-04T14:24:47.971607Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.865851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing zeroes: ['Unnamed: 0', 'is_stop', 'order', 'tfidf_art', 'label3', 'label4', 'label5', 'is_ne', 'negative_conc', 'positive_conc', 'weak_subj', 'strong_subj', 'hyperbolic_terms', 'attitude_markers', 'kill_verbs', 'bias_lexicon', 'assertive_verbs', 'factive_verbs', 'report_verbs', 'implicative_verbs', 'hedges', 'boosters', 'affect ', 'posemo ', 'negemo ', 'anx ', 'anger ', 'sad ', 'social ', 'family ', 'friend ', 'female ', 'male ', 'cogproc ', 'insight ', 'cause ', 'discrep ', 'tentat ', 'certain ', 'differ ', 'percept ', 'see ', 'hear ', 'feel ', 'bio ', 'body ', 'health ', 'sexual ', 'ingest ', 'drives ', 'affiliation ', 'achieve ', 'power ', 'reward ', 'risk ', 'focuspast ', 'focuspresent ', 'focusfuture ', 'relativ ', 'motion ', 'space ', 'time ', 'work ', 'leisure ', 'home ', 'money ', 'relig ', 'death ', 'informal ', 'swear ', 'netspeak ', 'assent ', 'nonflu ', 'filler ', 'pos_ADJ', 'pos_ADP', 'pos_ADV', 'pos_AUX', 'pos_DET', 'pos_INTJ', 'pos_NOUN', 'pos_PRON', 'pos_PROPN', 'pos_SCONJ', 'pos_VERB', 'pos_X', 'dep_ROOT', 'dep_acl', 'dep_acomp', 'dep_advcl', 'dep_advmod', 'dep_agent', 'dep_amod', 'dep_appos', 'dep_attr', 'dep_aux', 'dep_auxpass', 'dep_cc', 'dep_ccomp', 'dep_compound', 'dep_conj', 'dep_csubj', 'dep_dative', 'dep_dep', 'dep_det', 'dep_dobj', 'dep_expl', 'dep_intj', 'dep_mark', 'dep_neg', 'dep_nmod', 'dep_npadvmod', 'dep_nsubj', 'dep_nsubjpass', 'dep_nummod', 'dep_oprd', 'dep_parataxis', 'dep_pcomp', 'dep_pobj', 'dep_poss', 'dep_preconj', 'dep_predet', 'dep_prep', 'dep_prt', 'dep_punct', 'dep_quantmod', 'dep_relcl', 'dep_xcomp', 'ne_CARDINAL', 'ne_DATE', 'ne_EVENT', 'ne_FAC', 'ne_GPE', 'ne_LANGUAGE', 'ne_LAW', 'ne_LOC', 'ne_MONEY', 'ne_NORP', 'ne_ORDINAL', 'ne_ORG', 'ne_PERCENT', 'ne_PERSON', 'ne_PRODUCT', 'ne_QUANTITY', 'ne_TIME', 'ne_WORK_OF_ART', 'negative_conc_context', 'positive_conc_context', 'weak_subj_context', 'strong_subj_context', 'hyperbolic_terms_context', 'attitude_markers_context', 'kill_verbs_context', 'bias_lexicon_context', 'assertive_verbs_context', 'factive_verbs_context', 'report_verbs_context', 'implicative_verbs_context', 'hedges_context', 'boosters_context', 'affect _context', 'posemo _context', 'negemo _context', 'anx _context', 'anger _context', 'sad _context', 'social _context', 'family _context', 'friend _context', 'female _context', 'male _context', 'cogproc _context', 'insight _context', 'cause _context', 'discrep _context', 'tentat _context', 'certain _context', 'differ _context', 'percept _context', 'see _context', 'hear _context', 'feel _context', 'bio _context', 'body _context', 'health _context', 'sexual _context', 'ingest _context', 'drives _context', 'affiliation _context', 'achieve _context', 'power _context', 'reward _context', 'risk _context', 'focuspast _context', 'focuspresent _context', 'focusfuture _context', 'relativ _context', 'motion _context', 'space _context', 'time _context', 'work _context', 'leisure _context', 'home _context', 'money _context', 'relig _context', 'death _context', 'informal _context', 'swear _context', 'netspeak _context', 'assent _context', 'nonflu _context', 'filler _context', 'pos_ADJ_context', 'pos_ADP_context', 'pos_ADV_context', 'pos_AUX_context', 'pos_DET_context', 'pos_INTJ_context', 'pos_NOUN_context', 'pos_PRON_context', 'pos_PROPN_context', 'pos_SCONJ_context', 'pos_VERB_context', 'pos_X_context', 'dep_ROOT_context', 'dep_acl_context', 'dep_acomp_context', 'dep_advcl_context', 'dep_advmod_context', 'dep_agent_context', 'dep_amod_context', 'dep_appos_context', 'dep_attr_context', 'dep_aux_context', 'dep_auxpass_context', 'dep_cc_context', 'dep_ccomp_context', 'dep_compound_context', 'dep_conj_context', 'dep_csubj_context', 'dep_dative_context', 'dep_dep_context', 'dep_det_context', 'dep_dobj_context', 'dep_expl_context', 'dep_intj_context', 'dep_mark_context', 'dep_neg_context', 'dep_nmod_context', 'dep_npadvmod_context', 'dep_nsubj_context', 'dep_nsubjpass_context', 'dep_nummod_context', 'dep_oprd_context', 'dep_parataxis_context', 'dep_pcomp_context', 'dep_pobj_context', 'dep_poss_context', 'dep_preconj_context', 'dep_predet_context', 'dep_prep_context', 'dep_prt_context', 'dep_punct_context', 'dep_quantmod_context', 'dep_relcl_context', 'dep_xcomp_context', 'ne_CARDINAL_context', 'ne_DATE_context', 'ne_EVENT_context', 'ne_FAC_context', 'ne_GPE_context', 'ne_LAW_context', 'ne_LOC_context', 'ne_MONEY_context', 'ne_NORP_context', 'ne_ORDINAL_context', 'ne_ORG_context', 'ne_PERCENT_context', 'ne_PERSON_context', 'ne_PRODUCT_context', 'ne_QUANTITY_context', 'ne_TIME_context', 'ne_WORK_OF_ART_context', 'ne_LANGUAGE_context']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# identify which columns have one or more zeroes\n",
    "zero_columns = df.columns[(df == 0).any()].tolist()\n",
    "\n",
    "print(\"Columns containing zeroes:\", zero_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.974930Z",
     "iopub.status.busy": "2025-05-04T14:24:47.974613Z",
     "iopub.status.idle": "2025-05-04T14:24:47.991431Z",
     "shell.execute_reply": "2025-05-04T14:24:47.990255Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.974906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List the columns you want to process to fill since empty\n",
    "columns_to_fill = ['tfidf_art', 'MRCP_concretness_ratings', 'MRCP_Imagability_ratings',]\n",
    "\n",
    "# Fill NaN values in each column with that column's mean\n",
    "for col in columns_to_fill:\n",
    "    mean_val = df[col].mean()\n",
    "    df[col] = df[col].fillna(mean_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:47.994453Z",
     "iopub.status.busy": "2025-05-04T14:24:47.992715Z",
     "iopub.status.idle": "2025-05-04T14:24:48.251903Z",
     "shell.execute_reply": "2025-05-04T14:24:48.251043Z",
     "shell.execute_reply.started": "2025-05-04T14:24:47.994401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# columns to process\n",
    "columns_to_clean = ['ne_label', 'type']\n",
    "\n",
    "# Replace [''], [], or NaN with ['none'] in each column\n",
    "for col in columns_to_clean:\n",
    "    df[col] = df[col].apply(lambda x: ['none'] if (x == [''] or x == [] or pd.isna(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:48.252872Z",
     "iopub.status.busy": "2025-05-04T14:24:48.252656Z",
     "iopub.status.idle": "2025-05-04T14:24:48.264802Z",
     "shell.execute_reply": "2025-05-04T14:24:48.264080Z",
     "shell.execute_reply.started": "2025-05-04T14:24:48.252856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased_words2\n",
      "[]                              33655\n",
      "['claiming']                      439\n",
      "['claimed']                       316\n",
      "['claims']                        222\n",
      "['illegal', 'aliens']             209\n",
      "                                ...  \n",
      "['Firebombs']                       5\n",
      "['bungling']                        5\n",
      "['destructive']                     4\n",
      "['there', \"isn't\", 'anyone']        3\n",
      "['poison']                          3\n",
      "Name: count, Length: 1623, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['biased_words2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:48.266362Z",
     "iopub.status.busy": "2025-05-04T14:24:48.265965Z",
     "iopub.status.idle": "2025-05-04T14:24:48.512082Z",
     "shell.execute_reply": "2025-05-04T14:24:48.511208Z",
     "shell.execute_reply.started": "2025-05-04T14:24:48.266298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== File Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65822 entries, 0 to 65821\n",
      "Columns: 301 entries, Unnamed: 0 to ne_LANGUAGE_context\n",
      "dtypes: bool(2), float64(4), int64(281), object(14)\n",
      "memory usage: 150.3+ MB\n",
      "None\n",
      "\n",
      "=== Duplicates in 'text' ===\n",
      "Total duplicate values: 52531\n",
      "\n",
      "=== Zero Values ===\n",
      "Total zero values: 17437766\n",
      "\n",
      "=== Empty (NaN) Cells ===\n",
      "Total empty cells: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words2</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>pos</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32706</th>\n",
       "      <td>54621</td>\n",
       "      <td>No sooner do states start getting together to ...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>No sooner do states start getting together to ...</td>\n",
       "      <td>['assert']</td>\n",
       "      <td>start</td>\n",
       "      <td>start</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>7737</td>\n",
       "      <td>American Outdoor Brands Corp AOBC.O said on Th...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>gun control</td>\n",
       "      <td>center</td>\n",
       "      <td>American Outdoor Brands Corp AOBC.O said on Th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>said</td>\n",
       "      <td>said</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>3644</td>\n",
       "      <td>A Supreme Court filing lays bare the deep chas...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>taxes</td>\n",
       "      <td>left</td>\n",
       "      <td>A Supreme Court filing lays bare the deep chas...</td>\n",
       "      <td>['wannabe']</td>\n",
       "      <td>lays</td>\n",
       "      <td>lays</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55548</th>\n",
       "      <td>92804</td>\n",
       "      <td>This dark new event revealed that America’s wh...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>This dark new event revealed that America’s wh...</td>\n",
       "      <td>['dark']</td>\n",
       "      <td>America</td>\n",
       "      <td>america</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13535</th>\n",
       "      <td>22530</td>\n",
       "      <td>Conservative Republican Senator Ted Cruz favor...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>universal health care</td>\n",
       "      <td>center</td>\n",
       "      <td>Conservative Republican Senator Ted Cruz favor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>standards</td>\n",
       "      <td>standards</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           sentence  \\\n",
       "32706       54621  No sooner do states start getting together to ...   \n",
       "4645         7737  American Outdoor Brands Corp AOBC.O said on Th...   \n",
       "2217         3644  A Supreme Court filing lays bare the deep chas...   \n",
       "55548       92804  This dark new event revealed that America’s wh...   \n",
       "13535       22530  Conservative Republican Senator Ted Cruz favor...   \n",
       "\n",
       "         outlet                  topic    type  \\\n",
       "32706  Alternet            gun control    left   \n",
       "4645    Reuters            gun control  center   \n",
       "2217   Alternet                  taxes    left   \n",
       "55548  Alternet      white-nationalism    left   \n",
       "13535   Reuters  universal health care  center   \n",
       "\n",
       "                                                 article biased_words2  \\\n",
       "32706  No sooner do states start getting together to ...    ['assert']   \n",
       "4645   American Outdoor Brands Corp AOBC.O said on Th...            []   \n",
       "2217   A Supreme Court filing lays bare the deep chas...   ['wannabe']   \n",
       "55548  This dark new event revealed that America’s wh...      ['dark']   \n",
       "13535  Conservative Republican Senator Ted Cruz favor...            []   \n",
       "\n",
       "            text   text_low    pos  ... ne_NORP_context ne_ORDINAL_context  \\\n",
       "32706      start      start   VERB  ...               0                  0   \n",
       "4645        said       said   VERB  ...               0                  0   \n",
       "2217        lays       lays   VERB  ...               0                  0   \n",
       "55548    America    america  PROPN  ...               0                  0   \n",
       "13535  standards  standards   NOUN  ...               0                  0   \n",
       "\n",
       "      ne_ORG_context ne_PERCENT_context  ne_PERSON_context  \\\n",
       "32706              0                  0                  0   \n",
       "4645               1                  0                  0   \n",
       "2217               1                  0                  0   \n",
       "55548              0                  0                  0   \n",
       "13535              0                  0                  0   \n",
       "\n",
       "       ne_PRODUCT_context  ne_QUANTITY_context  ne_TIME_context  \\\n",
       "32706                   0                    0                0   \n",
       "4645                    0                    0                0   \n",
       "2217                    0                    0                0   \n",
       "55548                   0                    0                0   \n",
       "13535                   0                    0                0   \n",
       "\n",
       "       ne_WORK_OF_ART_context  ne_LANGUAGE_context  \n",
       "32706                       0                    0  \n",
       "4645                        0                    0  \n",
       "2217                        0                    0  \n",
       "55548                       0                    0  \n",
       "13535                       0                    0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check to see that all NAN cells have been filled. \n",
    "evaluate_file(df, duplicate_column=\"text\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:48.515117Z",
     "iopub.status.busy": "2025-05-04T14:24:48.514842Z",
     "iopub.status.idle": "2025-05-04T14:24:48.540349Z",
     "shell.execute_reply": "2025-05-04T14:24:48.539269Z",
     "shell.execute_reply.started": "2025-05-04T14:24:48.515087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words2</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>pos</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Orange</td>\n",
       "      <td>orange</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>New</td>\n",
       "      <td>new</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Black</td>\n",
       "      <td>black</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>star</td>\n",
       "      <td>star</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yael</td>\n",
       "      <td>yael</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Stone</td>\n",
       "      <td>stone</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>renouncing</td>\n",
       "      <td>renouncing</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>green</td>\n",
       "      <td>green</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>card</td>\n",
       "      <td>card</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>return</td>\n",
       "      <td>return</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>native</td>\n",
       "      <td>native</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Australia</td>\n",
       "      <td>australia</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>order</td>\n",
       "      <td>order</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>fight</td>\n",
       "      <td>fight</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>climate</td>\n",
       "      <td>climate</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>change</td>\n",
       "      <td>change</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>law</td>\n",
       "      <td>law</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>Trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>recently</td>\n",
       "      <td>recently</td>\n",
       "      <td>ADV</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>said</td>\n",
       "      <td>said</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>characteristically</td>\n",
       "      <td>characteristically</td>\n",
       "      <td>ADV</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>bizarre</td>\n",
       "      <td>bizarre</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>35</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>syntax</td>\n",
       "      <td>syntax</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>diction</td>\n",
       "      <td>diction</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>repeating</td>\n",
       "      <td>repeating</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>43</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>criminals</td>\n",
       "      <td>criminals</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>44</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>eugenics</td>\n",
       "      <td>eugenics</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>considered</td>\n",
       "      <td>considered</td>\n",
       "      <td>VERB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>51</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>fringe</td>\n",
       "      <td>fringe</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "      <td>extreme</td>\n",
       "      <td>extreme</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           sentence    outlet  \\\n",
       "0            0  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "1            3  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "2            4  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "3            5  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "4            6  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "5            7  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "6            9  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "7           11  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "8           12  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "9           13  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "10          14  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "11          17  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "12          18  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "13          20  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "14          21  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "15          22  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "16          23  \"Orange Is the New Black\" star Yael Stone is r...  Fox News   \n",
       "17          26  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "18          27  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "19          28  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "20          29  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "21          30  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "22          33  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "23          34  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "24          35  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "25          36  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "26          37  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "27          39  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "28          40  \"We have one beautiful law,\" Trump recently sa...  Alternet   \n",
       "29          41  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "30          43  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "31          44  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "32          50  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "33          51  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "34          52  ...immigrants as criminals and eugenics, all o...     MSNBC   \n",
       "\n",
       "                topic   type  \\\n",
       "0         environment  right   \n",
       "1         environment  right   \n",
       "2         environment  right   \n",
       "3         environment  right   \n",
       "4         environment  right   \n",
       "5         environment  right   \n",
       "6         environment  right   \n",
       "7         environment  right   \n",
       "8         environment  right   \n",
       "9         environment  right   \n",
       "10        environment  right   \n",
       "11        environment  right   \n",
       "12        environment  right   \n",
       "13        environment  right   \n",
       "14        environment  right   \n",
       "15        environment  right   \n",
       "16        environment  right   \n",
       "17        gun control   left   \n",
       "18        gun control   left   \n",
       "19        gun control   left   \n",
       "20        gun control   left   \n",
       "21        gun control   left   \n",
       "22        gun control   left   \n",
       "23        gun control   left   \n",
       "24        gun control   left   \n",
       "25        gun control   left   \n",
       "26        gun control   left   \n",
       "27        gun control   left   \n",
       "28        gun control   left   \n",
       "29  white-nationalism   left   \n",
       "30  white-nationalism   left   \n",
       "31  white-nationalism   left   \n",
       "32  white-nationalism   left   \n",
       "33  white-nationalism   left   \n",
       "34  white-nationalism   left   \n",
       "\n",
       "                                              article  \\\n",
       "0   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "1   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "2   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "3   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "4   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "5   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "6   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "7   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "8   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "9   \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "10  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "11  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "12  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "13  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "14  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "15  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "16  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "17  \"We have one beautiful law,\" Trump recently sa...   \n",
       "18  \"We have one beautiful law,\" Trump recently sa...   \n",
       "19  \"We have one beautiful law,\" Trump recently sa...   \n",
       "20  \"We have one beautiful law,\" Trump recently sa...   \n",
       "21  \"We have one beautiful law,\" Trump recently sa...   \n",
       "22  \"We have one beautiful law,\" Trump recently sa...   \n",
       "23  \"We have one beautiful law,\" Trump recently sa...   \n",
       "24  \"We have one beautiful law,\" Trump recently sa...   \n",
       "25  \"We have one beautiful law,\" Trump recently sa...   \n",
       "26  \"We have one beautiful law,\" Trump recently sa...   \n",
       "27  \"We have one beautiful law,\" Trump recently sa...   \n",
       "28  \"We have one beautiful law,\" Trump recently sa...   \n",
       "29  ...immigrants as criminals and eugenics, all o...   \n",
       "30  ...immigrants as criminals and eugenics, all o...   \n",
       "31  ...immigrants as criminals and eugenics, all o...   \n",
       "32  ...immigrants as criminals and eugenics, all o...   \n",
       "33  ...immigrants as criminals and eugenics, all o...   \n",
       "34  ...immigrants as criminals and eugenics, all o...   \n",
       "\n",
       "                         biased_words2                text  \\\n",
       "0                                   []              Orange   \n",
       "1                                   []                 New   \n",
       "2                                   []               Black   \n",
       "3                                   []                star   \n",
       "4                                   []                Yael   \n",
       "5                                   []               Stone   \n",
       "6                                   []          renouncing   \n",
       "7                                   []                U.S.   \n",
       "8                                   []               green   \n",
       "9                                   []                card   \n",
       "10                                  []              return   \n",
       "11                                  []              native   \n",
       "12                                  []           Australia   \n",
       "13                                  []               order   \n",
       "14                                  []               fight   \n",
       "15                                  []             climate   \n",
       "16                                  []              change   \n",
       "17   ['bizarre', 'characteristically']           beautiful   \n",
       "18   ['bizarre', 'characteristically']                 law   \n",
       "19   ['bizarre', 'characteristically']               Trump   \n",
       "20   ['bizarre', 'characteristically']            recently   \n",
       "21   ['bizarre', 'characteristically']                said   \n",
       "22   ['bizarre', 'characteristically']  characteristically   \n",
       "23   ['bizarre', 'characteristically']             bizarre   \n",
       "24   ['bizarre', 'characteristically']              syntax   \n",
       "25   ['bizarre', 'characteristically']             diction   \n",
       "26   ['bizarre', 'characteristically']           repeating   \n",
       "27   ['bizarre', 'characteristically']                word   \n",
       "28   ['bizarre', 'characteristically']           beautiful   \n",
       "29  ['criminals', 'fringe', 'extreme']          immigrants   \n",
       "30  ['criminals', 'fringe', 'extreme']           criminals   \n",
       "31  ['criminals', 'fringe', 'extreme']            eugenics   \n",
       "32  ['criminals', 'fringe', 'extreme']          considered   \n",
       "33  ['criminals', 'fringe', 'extreme']              fringe   \n",
       "34  ['criminals', 'fringe', 'extreme']             extreme   \n",
       "\n",
       "              text_low    pos  ... ne_NORP_context ne_ORDINAL_context  \\\n",
       "0               orange  PROPN  ...               0                  0   \n",
       "1                  new  PROPN  ...               0                  0   \n",
       "2                black  PROPN  ...               0                  0   \n",
       "3                 star   NOUN  ...               0                  0   \n",
       "4                 yael  PROPN  ...               0                  0   \n",
       "5                stone  PROPN  ...               0                  0   \n",
       "6           renouncing   VERB  ...               0                  0   \n",
       "7                 u.s.  PROPN  ...               0                  0   \n",
       "8                green    ADJ  ...               0                  0   \n",
       "9                 card   NOUN  ...               0                  0   \n",
       "10              return   VERB  ...               0                  0   \n",
       "11              native    ADJ  ...               0                  0   \n",
       "12           australia  PROPN  ...               0                  0   \n",
       "13               order   NOUN  ...               0                  0   \n",
       "14               fight   VERB  ...               0                  0   \n",
       "15             climate   NOUN  ...               0                  0   \n",
       "16              change   NOUN  ...               0                  0   \n",
       "17           beautiful    ADJ  ...               0                  0   \n",
       "18                 law   NOUN  ...               0                  0   \n",
       "19               trump  PROPN  ...               0                  0   \n",
       "20            recently    ADV  ...               0                  0   \n",
       "21                said   VERB  ...               0                  0   \n",
       "22  characteristically    ADV  ...               0                  0   \n",
       "23             bizarre    ADJ  ...               0                  0   \n",
       "24              syntax   NOUN  ...               0                  0   \n",
       "25             diction   NOUN  ...               0                  0   \n",
       "26           repeating   VERB  ...               0                  0   \n",
       "27                word   NOUN  ...               0                  0   \n",
       "28           beautiful    ADJ  ...               0                  0   \n",
       "29          immigrants   NOUN  ...               0                  0   \n",
       "30           criminals   NOUN  ...               0                  0   \n",
       "31            eugenics   NOUN  ...               0                  0   \n",
       "32          considered   VERB  ...               0                  0   \n",
       "33              fringe   NOUN  ...               0                  0   \n",
       "34             extreme   NOUN  ...               0                  0   \n",
       "\n",
       "   ne_ORG_context ne_PERCENT_context  ne_PERSON_context  ne_PRODUCT_context  \\\n",
       "0               0                  0                  0                   0   \n",
       "1               0                  0                  0                   0   \n",
       "2               0                  0                  1                   0   \n",
       "3               0                  0                  1                   0   \n",
       "4               0                  0                  1                   0   \n",
       "5               0                  0                  1                   0   \n",
       "6               0                  0                  1                   0   \n",
       "7               0                  0                  0                   0   \n",
       "8               0                  0                  0                   0   \n",
       "9               0                  0                  1                   0   \n",
       "10              0                  0                  1                   0   \n",
       "11              0                  0                  1                   0   \n",
       "12              0                  0                  0                   0   \n",
       "13              0                  0                  0                   0   \n",
       "14              0                  0                  0                   0   \n",
       "15              0                  0                  0                   0   \n",
       "16              0                  0                  0                   0   \n",
       "17              0                  0                  1                   0   \n",
       "18              0                  0                  1                   0   \n",
       "19              0                  0                  0                   0   \n",
       "20              0                  0                  1                   0   \n",
       "21              0                  0                  1                   0   \n",
       "22              0                  0                  0                   0   \n",
       "23              0                  0                  0                   0   \n",
       "24              0                  0                  0                   0   \n",
       "25              0                  0                  0                   0   \n",
       "26              0                  0                  0                   0   \n",
       "27              0                  0                  0                   0   \n",
       "28              0                  0                  0                   0   \n",
       "29              0                  0                  0                   0   \n",
       "30              0                  0                  0                   0   \n",
       "31              0                  0                  0                   0   \n",
       "32              0                  0                  0                   0   \n",
       "33              0                  0                  0                   0   \n",
       "34              0                  0                  0                   0   \n",
       "\n",
       "    ne_QUANTITY_context  ne_TIME_context  ne_WORK_OF_ART_context  \\\n",
       "0                     0                0                       1   \n",
       "1                     0                0                       1   \n",
       "2                     0                0                       1   \n",
       "3                     0                0                       1   \n",
       "4                     0                0                       1   \n",
       "5                     0                0                       0   \n",
       "6                     0                0                       0   \n",
       "7                     0                0                       0   \n",
       "8                     0                0                       0   \n",
       "9                     0                0                       0   \n",
       "10                    0                0                       0   \n",
       "11                    0                0                       0   \n",
       "12                    0                0                       0   \n",
       "13                    0                0                       0   \n",
       "14                    0                0                       0   \n",
       "15                    0                0                       0   \n",
       "16                    0                0                       0   \n",
       "17                    0                0                       0   \n",
       "18                    0                0                       0   \n",
       "19                    0                0                       0   \n",
       "20                    0                0                       0   \n",
       "21                    0                0                       0   \n",
       "22                    0                0                       0   \n",
       "23                    0                0                       0   \n",
       "24                    0                0                       0   \n",
       "25                    0                0                       0   \n",
       "26                    0                0                       0   \n",
       "27                    0                0                       0   \n",
       "28                    0                0                       0   \n",
       "29                    0                0                       0   \n",
       "30                    0                0                       0   \n",
       "31                    0                0                       0   \n",
       "32                    0                0                       0   \n",
       "33                    0                0                       0   \n",
       "34                    0                0                       0   \n",
       "\n",
       "    ne_LANGUAGE_context  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "5                     0  \n",
       "6                     0  \n",
       "7                     0  \n",
       "8                     0  \n",
       "9                     0  \n",
       "10                    0  \n",
       "11                    0  \n",
       "12                    0  \n",
       "13                    0  \n",
       "14                    0  \n",
       "15                    0  \n",
       "16                    0  \n",
       "17                    0  \n",
       "18                    0  \n",
       "19                    0  \n",
       "20                    0  \n",
       "21                    0  \n",
       "22                    0  \n",
       "23                    0  \n",
       "24                    0  \n",
       "25                    0  \n",
       "26                    0  \n",
       "27                    0  \n",
       "28                    0  \n",
       "29                    0  \n",
       "30                    0  \n",
       "31                    0  \n",
       "32                    0  \n",
       "33                    0  \n",
       "34                    0  \n",
       "\n",
       "[35 rows x 301 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:24:48.541495Z",
     "iopub.status.busy": "2025-05-04T14:24:48.541219Z",
     "iopub.status.idle": "2025-05-04T14:25:04.188174Z",
     "shell.execute_reply": "2025-05-04T14:25:04.187185Z",
     "shell.execute_reply.started": "2025-05-04T14:24:48.541467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain words: ['sentence', 'outlet', 'topic', 'type', 'article', 'biased_words2', 'text', 'text_low', 'pos', 'lemma', 'lemma_low', 'tag', 'dep', 'is_stop', 'is_ne', 'ne_label']\n"
     ]
    }
   ],
   "source": [
    "#find all columns with words\n",
    "def contains_words(series):\n",
    "    return series.astype(str).apply(lambda x: any(char.isalpha() for char in x)).any()\n",
    "\n",
    "word_columns = [col for col in df.columns if contains_words(df[col])]\n",
    "\n",
    "print(\"Columns that contain words:\", word_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:04.189682Z",
     "iopub.status.busy": "2025-05-04T14:25:04.189409Z",
     "iopub.status.idle": "2025-05-04T14:25:04.196770Z",
     "shell.execute_reply": "2025-05-04T14:25:04.195507Z",
     "shell.execute_reply.started": "2025-05-04T14:25:04.189664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                 int64\n",
      "sentence                  object\n",
      "outlet                    object\n",
      "topic                     object\n",
      "type                      object\n",
      "                           ...  \n",
      "ne_PRODUCT_context         int64\n",
      "ne_QUANTITY_context        int64\n",
      "ne_TIME_context            int64\n",
      "ne_WORK_OF_ART_context     int64\n",
      "ne_LANGUAGE_context        int64\n",
      "Length: 301, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check datatypes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:04.198095Z",
     "iopub.status.busy": "2025-05-04T14:25:04.197845Z",
     "iopub.status.idle": "2025-05-04T14:25:10.194335Z",
     "shell.execute_reply": "2025-05-04T14:25:10.193444Z",
     "shell.execute_reply.started": "2025-05-04T14:25:04.198076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns where any value is a list or string: ['sentence', 'outlet', 'topic', 'type', 'article', 'biased_words2', 'text', 'text_low', 'pos', 'lemma', 'lemma_low', 'tag', 'dep', 'ne_label']\n"
     ]
    }
   ],
   "source": [
    "#above is too broad so be more specific\n",
    "def contains_list_or_string(col):\n",
    "    return col.apply(lambda x: isinstance(x, (list, str))).any()\n",
    "\n",
    "list_or_string_columns = [col for col in df.columns if contains_list_or_string(df[col])]\n",
    "\n",
    "print(\"Columns where any value is a list or string:\", list_or_string_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:10.195629Z",
     "iopub.status.busy": "2025-05-04T14:25:10.195358Z",
     "iopub.status.idle": "2025-05-04T14:25:10.811535Z",
     "shell.execute_reply": "2025-05-04T14:25:10.810654Z",
     "shell.execute_reply.started": "2025-05-04T14:25:10.195608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List of columns to clean and encode\n",
    "columns_to_encode = list_or_string_columns\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "# Step 1: Normalize the data: convert lists to strings\n",
    "for col in columns_to_encode:\n",
    "    df[col] = df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Apply LabelEncoder\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str).fillna('unknown'))\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:10.813071Z",
     "iopub.status.busy": "2025-05-04T14:25:10.812796Z",
     "iopub.status.idle": "2025-05-04T14:25:17.274431Z",
     "shell.execute_reply": "2025-05-04T14:25:17.273508Z",
     "shell.execute_reply.started": "2025-05-04T14:25:10.813038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vectorize multiple text columns\n",
    "tfidf_vectorizers = {}\n",
    "tfidf_matrices = []\n",
    "\n",
    "for col in word_columns:\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "    matrix = tfidf.fit_transform(df[col].fillna('').astype(str))  # handle NaNs + force to string\n",
    "    tfidf_vectorizers[col] = tfidf\n",
    "    tfidf_matrices.append(matrix)\n",
    "\n",
    "# Combine all text vectors into one feature matrix\n",
    "from scipy.sparse import hstack\n",
    "X_text_combined = hstack(tfidf_matrices)\n",
    "\n",
    "#define target \n",
    "y = df['type'] \n",
    "#drop target so not leak into features\n",
    "df = df.drop('type', axis=1)\n",
    "\n",
    "# Pull out all encoded numeric columns\n",
    "encoded_features_array = df[[col + '_encoded' for col in columns_to_encode]].values\n",
    "\n",
    "# Combine sparse TF-IDF matrix with dense numeric matrix\n",
    "X_combined = hstack([X_text_combined, encoded_features_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:17.275845Z",
     "iopub.status.busy": "2025-05-04T14:25:17.275570Z",
     "iopub.status.idle": "2025-05-04T14:25:17.339238Z",
     "shell.execute_reply": "2025-05-04T14:25:17.338229Z",
     "shell.execute_reply.started": "2025-05-04T14:25:17.275826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_combined, y, test_size=0.4, random_state=54321)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=54321)\n",
    "\n",
    "# Assign \n",
    "features_train = X_train\n",
    "target_train = y_train\n",
    "\n",
    "features_valid = X_valid\n",
    "target_valid = y_valid\n",
    "\n",
    "features_test = X_test\n",
    "target_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define models and their respective parameter grids\n",
    "models_and_params = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'param_grid': {\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'param_grid': {}\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': xgb.XGBClassifier(),\n",
    "        'param_grid': {} \n",
    "    },\n",
    "    'LGBMClassifier': {\n",
    "        'model': lgb.LGBMClassifier(),\n",
    "        'param_grid': {}\n",
    "     },\n",
    "    'CatBoost': {\n",
    "        'model': CatBoostClassifier(),\n",
    "        'param_grid': {}\n",
    "    },\n",
    "}\n",
    "\n",
    "def train_models(models_and_params, features_train, target_train):\n",
    "    trained_models = {}\n",
    "\n",
    "    for model_name, config in models_and_params.items():\n",
    "        print(f\"\\nRunning GridSearchCV for {model_name}...\")\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['param_grid'],\n",
    "            cv=5,\n",
    "            scoring='f1_macro',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(features_train, target_train)\n",
    "\n",
    "        # Store the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        trained_models[model_name] = best_model\n",
    "        print(f\"Best {model_name} Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "def evaluate_models(trained_models, features_train, target_train, features_test, target_test):\n",
    "    for model_name, model in trained_models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        evaluate_classification_model(model_name, model, features_train, target_train, features_test, target_test)\n",
    "\n",
    "def evaluate_classification_model(model_name, model, features_train, target_train, features_test, target_test):\n",
    "    eval_stats = {}\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    for dataset_type, features, target in [('train', features_train, target_train), ('test', features_test, target_test)]:\n",
    "        eval_stats[dataset_type] = {}\n",
    "\n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]  # Use the correct dataset\n",
    "\n",
    "        # Compute metrics\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [f1_score(target, pred_proba >= threshold) for threshold in f1_thresholds]\n",
    "        fpr, tpr, _ = roc_curve(target, pred_proba)\n",
    "        precision, recall, _ = precision_recall_curve(target, pred_proba)\n",
    "\n",
    "        # Aggregate results\n",
    "        eval_stats[dataset_type]['Accuracy'] = accuracy_score(target, pred_target)\n",
    "        eval_stats[dataset_type]['F1'] = f1_score(target, pred_target)\n",
    "        eval_stats[dataset_type]['ROC AUC'] = roc_auc_score(target, pred_proba)\n",
    "        eval_stats[dataset_type]['APS'] = average_precision_score(target, pred_proba)\n",
    "\n",
    "        color = 'blue' if dataset_type == 'train' else 'green'\n",
    "\n",
    "        # F1 Score Plot\n",
    "        ax = axs[0]\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{dataset_type}, max={max(f1_scores):.2f}')\n",
    "        ax.set_xlabel('Threshold')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.legend()\n",
    "        ax.set_title(f'F1 Score ({model_name})')\n",
    "\n",
    "        # ROC Curve\n",
    "        ax = axs[1]\n",
    "        ax.plot(fpr, tpr, color=color, label=f'{dataset_type}, AUC={roc_auc_score(target, pred_proba):.2f}')\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.legend()\n",
    "        ax.set_title(f'ROC Curve ({model_name})')\n",
    "                # Precision-Recall Curve\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{dataset_type}, AP={average_precision_score(target, pred_proba):.2f}')\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.legend()\n",
    "        ax.set_title(f'Precision-Recall Curve ({model_name})')\n",
    "\n",
    "    df_eval_stats = pd.DataFrame(eval_stats).round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=['Accuracy', 'F1', 'APS', 'ROC AUC'])\n",
    "\n",
    "    print(df_eval_stats)\n",
    "    plt.show()\n",
    "\n",
    "# Run training and evaluation separately\n",
    "trained_models = train_models(models_and_params, features_train, target_train)\n",
    "evaluate_models(trained_models, features_train, target_train, features_test, target_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-04T14:27:21.771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_best_model_by_auc(trained_models, features_test, target_test):\n",
    "    \"\"\"Find the model with the highest ROC-AUC score on the test set.\"\"\"\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    best_auc = 0\n",
    "\n",
    "    for model_name, model in trained_models.items():\n",
    "        pred_proba = model.predict_proba(features_test)[:, 1]\n",
    "        auc = roc_auc_score(target_test, pred_proba)\n",
    "\n",
    "        print(f\"{model_name} Test ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model = model\n",
    "            best_model_name = model_name\n",
    "\n",
    "    print(f\"\\nBest Model: {best_model_name} with ROC-AUC: {best_auc:.4f}\")\n",
    "    return best_model, best_model_name\n",
    "\n",
    "# Find the best model based on ROC-AUC\n",
    "best_model, best_model_name = find_best_model_by_auc(trained_models, features_test, target_test)\n",
    "\n",
    "# Compute ROC-AUC on the validation set using the best model\n",
    "best_val_pred_proba = best_model.predict_proba(features_valid)[:, 1]\n",
    "auc_roc_val = roc_auc_score(target_valid, best_val_pred_proba)\n",
    "\n",
    "print(f\"AUC-ROC Score for Best Model ({best_model_name}) on Validation Set: {auc_roc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
